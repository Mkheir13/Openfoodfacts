import streamlit as st
import pandas as pd
import os
import numpy as np
from scripts.data.clean_dataset import process_and_analyze_dataset, display_cleaning_summary
from scripts.features.scaling import load_and_prepare_data, PRIORITY_KEYWORDS, VALID_RATIO_THRESHOLD
from scripts.models.clustering import prepare_features_for_clustering
from scripts.data.sample_large_files import get_reduced_file # Pour suggÃ©rer un fichier par dÃ©faut

# Configuration de la page Streamlit
st.set_page_config(layout="wide", page_title="Analyse et Clustering OpenFoodFacts")

st.title("Analyse et Clustering de DonnÃ©es OpenFoodFacts")

# Initialiser l'Ã©tat de session pour stocker les donnÃ©es
if 'preprocessed_data' not in st.session_state:
    st.session_state.preprocessed_data = None
if 'original_file_path' not in st.session_state:
    st.session_state.original_file_path = None
if 'preprocessed_file_path' not in st.session_state:
    st.session_state.preprocessed_file_path = None

# --- CrÃ©ation des onglets ---
tab1, tab2 = st.tabs(["ðŸ“Š Preprocessing", "ðŸ§  Clustering"])

# --- Onglet Preprocessing ---
with tab1:
    st.header("1. Chargement et Nettoyage des DonnÃ©es")

    # --- Section Chargement ---
    st.subheader("Chargement du fichier CSV")
    uploaded_file = st.file_uploader("Choisissez un fichier CSV", type="csv")

    # Suggestion du fichier Ã©chantillon s'il existe
    default_sample_path = "data/reduced/full_datas_100000.csv"
    if os.path.exists(default_sample_path):
        st.info(f"Fichier Ã©chantillon trouvÃ© : `{default_sample_path}`. Vous pouvez l'utiliser ou charger un autre fichier.")
        if st.button("Utiliser le fichier Ã©chantillon par dÃ©faut"):
             st.session_state.original_file_path = default_sample_path
             uploaded_file = None # Reset uploader if default is chosen
             st.success(f"Fichier `{default_sample_path}` sÃ©lectionnÃ©.")

    # Si un fichier est chargÃ© ou le chemin par dÃ©faut est sÃ©lectionnÃ©
    data_to_process = None
    if uploaded_file is not None:
        try:
            # Sauvegarder temporairement pour que clean_dataset puisse le lire (si nÃ©cessaire)
            # ou lire directement si clean_dataset accepte un BytesIO
            # Pour l'instant, on lit en DataFrame
            data_to_process = pd.read_csv(uploaded_file, sep='\t', low_memory=False) # Adapter sep si nÃ©cessaire
            st.session_state.original_file_path = uploaded_file.name # Garder trace du nom
            st.success(f"Fichier '{uploaded_file.name}' chargÃ© avec succÃ¨s.")
        except Exception as e:
            st.error(f"Erreur lors de la lecture du fichier CSV : {e}")
    elif st.session_state.original_file_path and os.path.exists(st.session_state.original_file_path):
         try:
            data_to_process = pd.read_csv(st.session_state.original_file_path, sep='\t', low_memory=False) # Adapter sep si nÃ©cessaire
            st.success(f"Fichier '{st.session_state.original_file_path}' chargÃ©.")
         except Exception as e:
            st.error(f"Erreur lors de la lecture du fichier {st.session_state.original_file_path} : {e}")


    if data_to_process is not None:
        st.dataframe(data_to_process.head())

        # --- Section Nettoyage ---
        st.subheader("Nettoyage et PrÃ©traitement")
        if st.button("Lancer le Preprocessing", key="preprocess_button"):
            with st.spinner("Nettoyage en cours... Veuillez patienter."):
                try:
                    # Simplifier pour utiliser directement load_and_prepare_data
                    with st.spinner("PrÃ©paration des donnÃ©es numÃ©riques..."):
                        # Nous avons besoin de sauvegarder temporairement le DataFrame
                        import tempfile
                        import os
                        
                        with tempfile.NamedTemporaryFile(suffix='.csv', delete=False) as temp_file:
                            temp_path = temp_file.name
                            # Sauver le DataFrame en CSV
                            data_to_process.to_csv(temp_path, index=False, sep='\t')
                        
                        try:
                            # Maintenant utiliser load_and_prepare_data avec ce fichier temporaire
                            df_numeric = load_and_prepare_data(temp_path)
                            # Supprimer le fichier temporaire
                            os.unlink(temp_path)
                            st.success(f"PrÃ©paration terminÃ©e: {len(df_numeric.columns)} colonnes numÃ©riques extraites")
                        except Exception as e:
                            # Supprimer le fichier temporaire en cas d'erreur
                            os.unlink(temp_path)
                            st.error(f"Erreur lors du preprocessing: {str(e)}")
                            raise e
                    
                    # Ã‰tape 2: Utiliser prepare_features_for_clustering pour le traitement avancÃ©
                    # ParamÃ¨tres configurables
                    max_categories = st.sidebar.slider("Nombre max de catÃ©gories", 5, 100, 30)
                    min_unique_ratio = st.sidebar.slider("Ratio min de valeurs uniques", 0.01, 0.30, 0.05)
                    
                    with st.spinner("PrÃ©paration des features pour clustering..."):
                        df_clean = prepare_features_for_clustering(
                            df_numeric,
                            max_categories=max_categories,
                            min_unique_ratio=min_unique_ratio
                        )
                        st.success("PrÃ©paration des features terminÃ©e !")

                    st.session_state.preprocessed_data = df_clean
                    st.success("Preprocessing complet terminÃ© !")

                    # Afficher le rÃ©sumÃ© du nettoyage
                    st.subheader("RÃ©sumÃ© du Nettoyage")
                    # CrÃ©er un rÃ©sumÃ© des opÃ©rations
                    cleaning_summary = {
                        "dimensions_initiales": data_to_process.shape,
                        "dimensions_finales": df_clean.shape,
                        "colonnes_supprimÃ©es": len(data_to_process.columns) - len(df_clean.columns),
                        "lignes_supprimÃ©es": data_to_process.shape[0] - df_clean.shape[0]
                    }
                    
                    st.write(f"Dimensions initiales : {cleaning_summary['dimensions_initiales']}")
                    st.write(f"Dimensions finales : {cleaning_summary['dimensions_finales']}")
                    st.write(f"Colonnes supprimÃ©es : {cleaning_summary['colonnes_supprimÃ©es']} colonnes")
                    st.write(f"Lignes supprimÃ©es : {cleaning_summary['lignes_supprimÃ©es']} lignes")

                    # Option de sauvegarde
                    st.subheader("Sauvegarder les donnÃ©es prÃ©traitÃ©es")
                    output_dir = "data/preprocessed"
                    os.makedirs(output_dir, exist_ok=True)
                    # Construire un nom de fichier unique basÃ© sur l'original
                    base_name = os.path.splitext(os.path.basename(st.session_state.original_file_path))[0]
                    preprocessed_filename = f"{base_name}_preprocessed.csv"
                    st.session_state.preprocessed_file_path = os.path.join(output_dir, preprocessed_filename)

                    try:
                        df_clean.to_csv(st.session_state.preprocessed_file_path, index=False, sep='\t') # ou ',' selon prÃ©fÃ©rence
                        st.success(f"DonnÃ©es prÃ©traitÃ©es sauvegardÃ©es dans : `{st.session_state.preprocessed_file_path}`")

                        # Bouton de tÃ©lÃ©chargement
                        with open(st.session_state.preprocessed_file_path, "rb") as fp:
                             st.download_button(
                                 label="TÃ©lÃ©charger le CSV PrÃ©traitÃ©",
                                 data=fp,
                                 file_name=preprocessed_filename,
                                 mime="text/csv"
                             )
                    except Exception as e:
                        st.error(f"Erreur lors de la sauvegarde du fichier : {e}")


                except Exception as e:
                    st.error(f"Une erreur est survenue lors du preprocessing : {e}")
                    st.exception(e) # Affiche la trace complÃ¨te pour le dÃ©bogage

    # Afficher les donnÃ©es prÃ©traitÃ©es si elles existent
    if st.session_state.preprocessed_data is not None:
        st.subheader("AperÃ§u des donnÃ©es prÃ©traitÃ©es")
        st.dataframe(st.session_state.preprocessed_data.head())
        st.info("Les donnÃ©es prÃ©traitÃ©es sont prÃªtes. Vous pouvez passer Ã  l'onglet 'Clustering'.")


# --- Onglet Clustering ---
with tab2:
    st.header("2. Clustering des DonnÃ©es")

    if st.session_state.preprocessed_data is None:
        st.warning("Veuillez d'abord charger et prÃ©traiter des donnÃ©es dans l'onglet 'Preprocessing'.")
        # Option pour charger un fichier prÃ©-traitÃ© existant ?
        st.subheader("Ou charger un fichier prÃ©-traitÃ© existant")
        uploaded_preprocessed_file = st.file_uploader("Choisissez un fichier CSV prÃ©-traitÃ©", type="csv", key="preprocessed_uploader")

        if uploaded_preprocessed_file is not None:
             try:
                 # Lire directement le fichier uploadÃ©
                 st.session_state.preprocessed_data = pd.read_csv(uploaded_preprocessed_file, sep='\t', low_memory=False) # Adapter sep si nÃ©cessaire
                 # Garder une trace du nom, pas forcÃ©ment le chemin complet
                 st.session_state.preprocessed_file_path = uploaded_preprocessed_file.name 
                 st.success(f"DonnÃ©es prÃ©-traitÃ©es chargÃ©es depuis {uploaded_preprocessed_file.name}")
                 # RafraÃ®chir la page pour que le reste de l'onglet se mette Ã  jour avec les donnÃ©es chargÃ©es
                 st.rerun()
             except Exception as e:
                 st.error(f"Erreur lors du chargement du fichier prÃ©-traitÃ© : {e}")


    if st.session_state.preprocessed_data is not None:
        st.success("DonnÃ©es prÃ©traitÃ©es disponibles.")
        df_cluster = st.session_state.preprocessed_data

        st.dataframe(df_cluster.head())

        # --- SÃ©lection des colonnes ---
        st.subheader("SÃ©lection des colonnes pour le clustering")
        # Identifier les colonnes numÃ©riques potentielles
        numeric_cols = df_cluster.select_dtypes(include=np.number).columns.tolist()
        # Exclure les identifiants ou colonnes non pertinentes si possible (peut nÃ©cessiter une heuristique)
        cols_to_exclude_heuristic = ['code', 'id', 'Unnamed: 0'] # A adapter
        available_cols = [col for col in numeric_cols if col not in cols_to_exclude_heuristic]

        if not available_cols:
             st.warning("Aucune colonne numÃ©rique appropriÃ©e trouvÃ©e pour le clustering aprÃ¨s le preprocessing.")
        else:
            selected_cols = st.multiselect(
                "Choisissez les colonnes numÃ©riques Ã  utiliser :",
                options=available_cols,
                default=available_cols[:min(5, len(available_cols))] # SÃ©lectionner les 5 premiÃ¨res par dÃ©faut
            )

            if not selected_cols:
                st.warning("Veuillez sÃ©lectionner au moins une colonne pour le clustering.")
            else:
                st.write("Colonnes sÃ©lectionnÃ©es:", selected_cols)
                data_for_clustering = df_cluster[selected_cols].dropna() # Supprimer les NA restantes sur les colonnes sÃ©lectionnÃ©es

                if data_for_clustering.empty:
                    st.error("AprÃ¨s suppression des valeurs manquantes sur les colonnes sÃ©lectionnÃ©es, il ne reste plus de donnÃ©es pour le clustering.")
                else:
                    st.info(f"{len(data_for_clustering)} lignes utilisables pour le clustering aprÃ¨s suppression des NA.")

                    # --- Choix de l'algorithme et des hyperparamÃ¨tres ---
                    st.subheader("Configuration du Clustering")
                    algo = st.selectbox("Choisissez l'algorithme de clustering :", ["K-Means", "DBSCAN", "GMM"]) # Ajout de GMM

                    params = {}
                    if algo == "K-Means":
                        # Partie 1: Configuration de l'optimisation du nombre de clusters
                        st.subheader("Optimisation du nombre de clusters")
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            params['k_min'] = st.number_input("Nombre minimum de clusters Ã  tester", 
                                                           min_value=2, 
                                                           max_value=10, 
                                                           value=2)
                        with col2:
                            params['k_max'] = st.number_input("Nombre maximum de clusters Ã  tester", 
                                                           min_value=3, 
                                                           max_value=30, 
                                                           value=15)
                        
                        # MÃ©thodes de scoring pour l'optimisation
                        scoring_methods = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'inertia']
                        params['scoring_method'] = st.selectbox(
                            "MÃ©thode d'Ã©valuation pour le nombre optimal de clusters", 
                            options=scoring_methods,
                            index=0,  # silhouette par dÃ©faut
                            help="Silhouette (plus Ã©levÃ© = meilleur), Calinski-Harabasz (plus Ã©levÃ© = meilleur), Davies-Bouldin (plus bas = meilleur), Inertia (plus bas = meilleur)"
                        )
                        
                        params['n_init'] = st.slider("Nombre d'initialisations pour chaque valeur de k", 
                                                 min_value=1, 
                                                 max_value=20, 
                                                 value=10)
                        
                        # Partie 2: Configuration de l'entraÃ®nement K-means
                        st.subheader("Configuration de l'entraÃ®nement K-means")
                        
                        # Option pour utiliser le nombre optimal ou dÃ©finir manuellement
                        use_optimal = st.checkbox("Rechercher automatiquement le nombre optimal de clusters", value=True)
                        if not use_optimal:
                            params['n_clusters'] = st.number_input(
                                "Nombre de clusters", 
                                min_value=2, 
                                max_value=50, 
                                value=5
                            )
                        else:
                            params['n_clusters'] = None
                        
                        # MÃ©thodes d'optimisation pour K-means
                        optimize_methods = ['multiple_init', 'grid_search', 'elkan']
                        params['optimize_method'] = st.selectbox(
                            "MÃ©thode d'optimisation des paramÃ¨tres K-means", 
                            options=optimize_methods,
                            index=0  # multiple_init par dÃ©faut
                        )
                        
                        # ParamÃ¨tres spÃ©cifiques selon la mÃ©thode d'optimisation
                        if params['optimize_method'] == 'multiple_init':
                            col1, col2 = st.columns(2)
                            with col1:
                                params['max_iter'] = st.slider(
                                    "Nombre maximum d'itÃ©rations", 
                                    min_value=100, 
                                    max_value=1000, 
                                    value=300, 
                                    step=100
                                )
                            with col2:
                                params['init'] = st.selectbox(
                                    "MÃ©thode d'initialisation", 
                                    options=['k-means++', 'random'],
                                    index=0
                                )
                        elif params['optimize_method'] == 'grid_search':
                            params['cv'] = st.slider(
                                "Nombre de folds pour validation croisÃ©e", 
                                min_value=2, 
                                max_value=10, 
                                value=3
                            )
                        
                        # ParamÃ¨tres de sauvegarde
                        params['save'] = st.checkbox("Sauvegarder le modÃ¨le entraÃ®nÃ©", value=True)
                        if params['save']:
                            params['model_path'] = st.text_input(
                                "Chemin pour sauvegarder le modÃ¨le", 
                                value="models/kmeans_model.pkl"
                            )
                        
                        # Forcer optimize=True si n_clusters=None
                        params['optimize'] = True if params['n_clusters'] is None else False
                        
                    elif algo == "DBSCAN":
                        params['eps'] = st.slider("Epsilon (eps)", min_value=0.1, max_value=5.0, value=0.5, step=0.1)
                        params['min_samples'] = st.slider("Nombre minimum de points (min_samples)", min_value=2, max_value=50, value=5, step=1)
                        # Ajouter d'autres paramÃ¨tres DBSCAN (metric, algorithm)
                        params['optimize'] = False # Ne pas optimiser par dÃ©faut dans l'UI, utiliser les valeurs fournies
                    
                    elif algo == "GMM":
                        # Partie 1: Configuration de l'optimisation du nombre de composantes
                        st.subheader("Optimisation du nombre de composantes")
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            params['n_min'] = st.number_input("Nombre minimum de composantes Ã  tester", 
                                                          min_value=2, 
                                                          max_value=10, 
                                                          value=2)
                        with col2:
                            params['n_max'] = st.number_input("Nombre maximum de composantes Ã  tester", 
                                                          min_value=3, 
                                                          max_value=30, 
                                                          value=15)
                        
                        # MÃ©thodes de scoring pour l'optimisation
                        scoring_methods = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'bic']
                        params['scoring_method'] = st.selectbox(
                            "MÃ©thode d'Ã©valuation pour le nombre optimal de composantes", 
                            options=scoring_methods,
                            index=0,  # silhouette par dÃ©faut
                            help="Silhouette (plus Ã©levÃ© = meilleur), Calinski-Harabasz (plus Ã©levÃ© = meilleur), Davies-Bouldin (plus bas = meilleur), BIC (plus bas = meilleur)",
                            key="gmm_scoring_method"
                        )
                        
                        params['n_init'] = st.slider("Nombre d'initialisations pour chaque valeur de n", 
                                                 min_value=1, 
                                                 max_value=20, 
                                                 value=10,
                                                 key="gmm_n_init")
                        
                        # Partie 2: Configuration de l'entraÃ®nement GMM
                        st.subheader("Configuration de l'entraÃ®nement GMM")
                        
                        # Option pour utiliser le nombre optimal ou dÃ©finir manuellement
                        use_optimal = st.checkbox("Rechercher automatiquement le nombre optimal de composantes", 
                                              value=True,
                                              key="gmm_use_optimal")
                        if not use_optimal:
                            params['n_components'] = st.number_input(
                                "Nombre de composantes", 
                                min_value=2, 
                                max_value=50, 
                                value=5,
                                key="gmm_n_components"
                            )
                        else:
                            params['n_components'] = None
                        
                        # MÃ©thodes d'optimisation pour GMM
                        optimize_methods = ['multiple_init', 'grid_search', 'covariance_type']
                        params['optimize_method'] = st.selectbox(
                            "MÃ©thode d'optimisation des paramÃ¨tres GMM", 
                            options=optimize_methods,
                            index=0,  # multiple_init par dÃ©faut
                            key="gmm_optimize_method"
                        )
                        
                        # ParamÃ¨tres spÃ©cifiques selon la mÃ©thode d'optimisation
                        if params['optimize_method'] == 'multiple_init':
                            col1, col2 = st.columns(2)
                            with col1:
                                params['max_iter'] = st.slider(
                                    "Nombre maximum d'itÃ©rations", 
                                    min_value=50, 
                                    max_value=500, 
                                    value=100, 
                                    step=50,
                                    key="gmm_max_iter"
                                )
                            with col2:
                                params['init_params'] = st.selectbox(
                                    "MÃ©thode d'initialisation", 
                                    options=['kmeans', 'random'],
                                    index=0,
                                    key="gmm_init_params"
                                )
                        elif params['optimize_method'] == 'grid_search':
                            params['cv'] = st.slider(
                                "Nombre de folds pour validation croisÃ©e", 
                                min_value=2, 
                                max_value=10, 
                                value=3,
                                key="gmm_cv"
                            )
                        elif params['optimize_method'] == 'covariance_type':
                            params['cov_types'] = st.multiselect(
                                "Types de matrices de covariance Ã  tester",
                                options=['full', 'tied', 'diag', 'spherical'],
                                default=['full', 'tied', 'diag', 'spherical'],
                                key="gmm_cov_types"
                            )
                        
                        # ParamÃ¨tres de sauvegarde
                        params['save'] = st.checkbox("Sauvegarder le modÃ¨le entraÃ®nÃ©", 
                                                 value=True,
                                                 key="gmm_save")
                        if params['save']:
                            params['model_path'] = st.text_input(
                                "Chemin pour sauvegarder le modÃ¨le", 
                                value="models/gmm_model.pkl",
                                key="gmm_model_path"
                            )
                        
                        # Forcer optimize=True si n_components=None
                        params['optimize'] = True if params['n_components'] is None else False


                    # --- Bouton pour lancer le clustering ---
                    if st.button(f"Lancer le Clustering {algo}", key=f"cluster_{algo}"):
                        with st.spinner(f"Clustering {algo} en cours..."):
                            try:
                                # Importer dynamiquement ou avoir les imports en haut
                                from sklearn.preprocessing import StandardScaler
                                # Importer la classe ModelComparison
                                from scripts.models.model_comparison import ModelComparison
                                import matplotlib.pyplot as plt
                                from sklearn.decomposition import PCA
                                import time

                                # 1. Standardiser les donnÃ©es sÃ©lectionnÃ©es
                                scaler = StandardScaler()
                                scaled_data = scaler.fit_transform(data_for_clustering)
                                st.write("DonnÃ©es standardisÃ©es pour le clustering.")

                                # 2. Initialiser et entraÃ®ner le modÃ¨le avec ModelComparison
                                start_time = time.time()
                                
                                # CrÃ©er une instance de ModelComparison
                                model_comp = ModelComparison(data_for_clustering)
                                
                                # PrÃ©paration des paramÃ¨tres pour chaque algorithme
                                all_params = {}
                                
                                if algo == "K-Means":
                                    # PrÃ©paration de dictionnaires distincts pour Ã©viter les conflits de paramÃ¨tres
                                    # 1. ParamÃ¨tres pour find_optimal_clusters
                                    find_optimal_params = {
                                        'k_min': params['k_min'],
                                        'k_max': params['k_max'],
                                        'method': params['scoring_method'],
                                        'n_init': params['n_init']
                                    }
                                    
                                    # 2. ParamÃ¨tres pour optimize_kmeans
                                    optimize_params = {
                                        'method': params['optimize_method']
                                    }
                                    
                                    # Ajouter des paramÃ¨tres spÃ©cifiques selon la mÃ©thode d'optimisation
                                    if params['optimize_method'] == 'multiple_init':
                                        optimize_params.update({
                                            'max_iter': params.get('max_iter', 300),
                                            'init': params.get('init', 'k-means++')
                                        })
                                    elif params['optimize_method'] == 'grid_search':
                                        optimize_params.update({
                                            'cv': params.get('cv', 3)
                                        })
                                    
                                    # 3. ParamÃ¨tres gÃ©nÃ©raux pour train_kmeans
                                    kmeans_params = {
                                        'n_clusters': params['n_clusters'],
                                        'optimize': params['optimize'],
                                        'save': params.get('save', True)
                                    }
                                    
                                    # Ajouter le chemin du modÃ¨le si sauvegarde activÃ©e
                                    if params.get('save', True) and 'model_path' in params:
                                        kmeans_params['model_path'] = params['model_path']
                                    
                                    # Import direct pour Ã©viter les problÃ¨mes de paramÃ¨tres
                                    from scripts.models.kmeans_training import train_kmeans, find_optimal_clusters
                                    
                                    # Si recherche automatique du nombre optimal, afficher le graphique d'analyse
                                    if params['optimize'] and params['n_clusters'] is None:
                                        with st.spinner("Recherche du nombre optimal de clusters..."):
                                            # Calcul des scores pour diffÃ©rents nombres de clusters
                                            scores = find_optimal_clusters(
                                                scaled_data,
                                                **find_optimal_params
                                            )
                                            
                                            # Affichage du graphique d'analyse
                                            st.subheader(f"Analyse du nombre optimal de clusters - {params['scoring_method']}")
                                            fig, ax = plt.subplots(figsize=(10, 6))
                                            ax.plot(list(scores.keys()), list(scores.values()), marker='o')
                                            ax.set_xlabel('Nombre de clusters')
                                            ax.set_ylabel('Score')
                                            ax.set_title(f'Analyse du nombre optimal de clusters - {params["scoring_method"]}')
                                            ax.grid(True)
                                            st.pyplot(fig)
                                            
                                            # SÃ©lection du nombre optimal
                                            optimal_k = max(scores.items(), key=lambda x: x[1])[0]
                                            st.success(f"Nombre optimal de clusters trouvÃ©: {optimal_k}")
                                    
                                    # EntraÃ®nement du modÃ¨le
                                    with st.spinner("EntraÃ®nement K-Means en cours..."):
                                        # CrÃ©er une version personnalisÃ©e de train_kmeans pour Ã©viter les conflits
                                        def custom_train_kmeans(data, n_clusters=None, optimize=True, save=True,
                                                             k_min=2, k_max=10, scoring_method='silhouette', n_init=10,
                                                             optimize_method='multiple_init', model_path='models/kmeans_model.pkl',
                                                             max_iter=300, init='k-means++', cv=3):
                                            """Version personnalisÃ©e de train_kmeans pour Ã©viter les conflits de paramÃ¨tres"""
                                            from sklearn.preprocessing import StandardScaler
                                            from sklearn.cluster import KMeans
                                            from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
                                            import matplotlib.pyplot as plt
                                            import os
                                            import joblib
                                            import numpy as np
                                            import time
                                            
                                            # Standardisation des donnÃ©es
                                            scaler = StandardScaler()
                                            scaled_data = scaler.fit_transform(data)
                                            
                                            # Si on doit trouver le nombre optimal de clusters
                                            if optimize and n_clusters is None:
                                                scores = {}
                                                
                                                # Ã‰valuer diffÃ©rents nombres de clusters
                                                for k in range(k_min, k_max + 1):
                                                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=n_init)
                                                    labels = kmeans.fit_predict(scaled_data)
                                                    
                                                    # Calculer le score selon la mÃ©thode choisie
                                                    if scoring_method == 'silhouette':
                                                        scores[k] = silhouette_score(scaled_data, labels)
                                                    elif scoring_method == 'calinski_harabasz':
                                                        scores[k] = calinski_harabasz_score(scaled_data, labels)
                                                    elif scoring_method == 'davies_bouldin':
                                                        scores[k] = -davies_bouldin_score(scaled_data, labels)
                                                    elif scoring_method == 'inertia':
                                                        scores[k] = -kmeans.inertia_
                                                
                                                # Trouver le meilleur k
                                                n_clusters = max(scores.items(), key=lambda x: x[1])[0]
                                                st.success(f"Nombre optimal de clusters dÃ©terminÃ©: {n_clusters}")
                                            
                                            # EntraÃ®ner le modÃ¨le final avec les meilleurs paramÃ¨tres
                                            best_model = None
                                            
                                            if optimize_method == 'multiple_init':
                                                best_score = -np.inf
                                                for _ in range(n_init):
                                                    kmeans = KMeans(
                                                        n_clusters=n_clusters,
                                                        init=init,
                                                        max_iter=max_iter,
                                                        random_state=None
                                                    )
                                                    kmeans.fit(scaled_data)
                                                    
                                                    score = silhouette_score(scaled_data, kmeans.labels_)
                                                    if score > best_score:
                                                        best_score = score
                                                        best_model = kmeans
                                            
                                            elif optimize_method == 'grid_search':
                                                from sklearn.model_selection import GridSearchCV
                                                
                                                param_grid = {
                                                    'init': ['k-means++', 'random'],
                                                    'n_init': [10, 20],
                                                    'max_iter': [200, 300, 400],
                                                    'algorithm': ['elkan', 'full']
                                                }
                                                
                                                kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                                                grid_search = GridSearchCV(
                                                    kmeans,
                                                    param_grid=param_grid,
                                                    cv=cv,
                                                    scoring='silhouette'
                                                )
                                                grid_search.fit(scaled_data)
                                                best_model = grid_search.best_estimator_
                                            
                                            elif optimize_method == 'elkan':
                                                best_score = -np.inf
                                                for algo in ['elkan', 'full']:
                                                    kmeans = KMeans(
                                                        n_clusters=n_clusters,
                                                        algorithm=algo,
                                                        random_state=42
                                                    )
                                                    kmeans.fit(scaled_data)
                                                    
                                                    score = silhouette_score(scaled_data, kmeans.labels_)
                                                    if score > best_score:
                                                        best_score = score
                                                        best_model = kmeans
                                            
                                            else:
                                                # MÃ©thode par dÃ©faut si aucune des mÃ©thodes spÃ©cifiÃ©es n'est choisie
                                                best_model = KMeans(
                                                    n_clusters=n_clusters,
                                                    init=init,
                                                    n_init=n_init,
                                                    random_state=42
                                                ).fit(scaled_data)
                                            
                                            # Sauvegarder le modÃ¨le si demandÃ©
                                            if save:
                                                os.makedirs(os.path.dirname(model_path), exist_ok=True)
                                                scaler_path = os.path.join(os.path.dirname(model_path), 'scaler.pkl')
                                                
                                                joblib.dump(best_model, model_path)
                                                joblib.dump(scaler, scaler_path)
                                                st.info(f"ModÃ¨le sauvegardÃ© dans {model_path}")
                                            
                                            # CrÃ©er le dictionnaire d'informations
                                            labels = best_model.labels_
                                            info = {
                                                'n_clusters': n_clusters,
                                                'optimize_method': optimize_method,
                                                'scaler': scaler,
                                                'silhouette_score': silhouette_score(scaled_data, labels),
                                                'calinski_harabasz_score': calinski_harabasz_score(scaled_data, labels),
                                                'davies_bouldin_score': davies_bouldin_score(scaled_data, labels),
                                                'inertia': best_model.inertia_
                                            }
                                            
                                            return best_model, info
                                        
                                        # Appel de notre fonction personnalisÃ©e
                                        # On passe les paramÃ¨tres avec leurs noms explicites pour Ã©viter tout conflit
                                        model, info_dict = custom_train_kmeans(
                                            data=data_for_clustering,
                                            n_clusters=params['n_clusters'],
                                            optimize=params['optimize'],
                                            save=params.get('save', True),
                                            k_min=find_optimal_params['k_min'],
                                            k_max=find_optimal_params['k_max'],
                                            scoring_method=find_optimal_params['method'],
                                            n_init=find_optimal_params['n_init'],
                                            optimize_method=optimize_params['method'],
                                            model_path=kmeans_params.get('model_path', 'models/kmeans_model.pkl'),
                                            max_iter=optimize_params.get('max_iter', 300),
                                            init=optimize_params.get('init', 'k-means++'),
                                            cv=optimize_params.get('cv', 3)
                                        )
                                        
                                        # Mise Ã  jour des attributs de modÃ¨le
                                        model_comp.models['kmeans'] = model
                                        info = info_dict
                                        info['training_time'] = time.time() - start_time
                                        model_comp.results['kmeans'] = info
                                        
                                        labels = model.labels_
                                
                                elif algo == "DBSCAN":
                                    dbscan_params = {
                                        'eps': params['eps'],
                                        'min_samples': params['min_samples'],
                                        'optimize': params['optimize']
                                    }
                                    all_params['dbscan'] = dbscan_params
                                    
                                    # Appel spÃ©cifique Ã  train_dbscan
                                    info = model_comp.train_dbscan(**dbscan_params)
                                    model = model_comp.models['dbscan']
                                    labels = model.labels_
                                
                                elif algo == "GMM":
                                    # CrÃ©er une version personnalisÃ©e de train_gmm pour Ã©viter les conflits
                                    def custom_train_gmm(data, n_components=None, optimize=True, save=True,
                                                     n_min=2, n_max=10, scoring_method='silhouette', n_init=10,
                                                     optimize_method='multiple_init', model_path='models/gmm_model.pkl',
                                                     max_iter=100, init_params='kmeans', cv=3, cov_types=None):
                                        """Version personnalisÃ©e de train_gmm pour Ã©viter les conflits de paramÃ¨tres"""
                                        from sklearn.preprocessing import StandardScaler
                                        from sklearn.mixture import GaussianMixture
                                        from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
                                        import matplotlib.pyplot as plt
                                        import os
                                        import joblib
                                        import numpy as np
                                        import time
                                        
                                        # Standardisation des donnÃ©es
                                        scaler = StandardScaler()
                                        scaled_data = scaler.fit_transform(data)
                                        
                                        # Si on doit trouver le nombre optimal de composantes
                                        if optimize and n_components is None:
                                            scores = {}
                                            
                                            # Ã‰valuer diffÃ©rents nombres de composantes
                                            for n in range(n_min, n_max + 1):
                                                gmm = GaussianMixture(n_components=n, random_state=42)
                                                labels = gmm.fit_predict(scaled_data)
                                                
                                                # Calculer le score selon la mÃ©thode choisie
                                                if scoring_method == 'silhouette':
                                                    scores[n] = silhouette_score(scaled_data, labels)
                                                elif scoring_method == 'calinski_harabasz':
                                                    scores[n] = calinski_harabasz_score(scaled_data, labels)
                                                elif scoring_method == 'davies_bouldin':
                                                    scores[n] = -davies_bouldin_score(scaled_data, labels)
                                                elif scoring_method == 'bic':
                                                    scores[n] = -gmm.bic(scaled_data)
                                            
                                            # Trouver le meilleur n
                                            n_components = max(scores.items(), key=lambda x: x[1])[0]
                                            st.success(f"Nombre optimal de composantes dÃ©terminÃ©: {n_components}")
                                        
                                        # EntraÃ®ner le modÃ¨le final avec les meilleurs paramÃ¨tres
                                        best_model = None
                                        
                                        if optimize_method == 'multiple_init':
                                            best_score = -np.inf
                                            for _ in range(n_init):
                                                gmm = GaussianMixture(
                                                    n_components=n_components,
                                                    init_params=init_params,
                                                    max_iter=max_iter,
                                                    random_state=None
                                                )
                                                gmm.fit(scaled_data)
                                                
                                                score = silhouette_score(scaled_data, gmm.predict(scaled_data))
                                                if score > best_score:
                                                    best_score = score
                                                    best_model = gmm
                                        
                                        elif optimize_method == 'grid_search':
                                            from sklearn.model_selection import GridSearchCV
                                            
                                            param_grid = {
                                                'init_params': ['kmeans', 'random'],
                                                'n_init': [10, 20],
                                                'max_iter': [100, 200],
                                                'covariance_type': ['full', 'tied', 'diag', 'spherical']
                                            }
                                            
                                            gmm = GaussianMixture(n_components=n_components, random_state=42)
                                            grid_search = GridSearchCV(
                                                gmm,
                                                param_grid=param_grid,
                                                cv=cv,
                                                scoring='silhouette'
                                            )
                                            grid_search.fit(scaled_data)
                                            best_model = grid_search.best_estimator_
                                        
                                        elif optimize_method == 'covariance_type':
                                            best_score = -np.inf
                                            types = cov_types if cov_types else ['full', 'tied', 'diag', 'spherical']
                                            for cov_type in types:
                                                gmm = GaussianMixture(
                                                    n_components=n_components,
                                                    covariance_type=cov_type,
                                                    random_state=42
                                                )
                                                gmm.fit(scaled_data)
                                                
                                                score = silhouette_score(scaled_data, gmm.predict(scaled_data))
                                                if score > best_score:
                                                    best_score = score
                                                    best_model = gmm
                                        
                                        else:
                                            # MÃ©thode par dÃ©faut
                                            best_model = GaussianMixture(
                                                n_components=n_components,
                                                init_params=init_params,
                                                max_iter=max_iter,
                                                random_state=42
                                            ).fit(scaled_data)
                                        
                                        # Sauvegarder le modÃ¨le si demandÃ©
                                        if save:
                                            os.makedirs(os.path.dirname(model_path), exist_ok=True)
                                            scaler_path = os.path.join(os.path.dirname(model_path), 'scaler.pkl')
                                            
                                            joblib.dump(best_model, model_path)
                                            joblib.dump(scaler, scaler_path)
                                            st.info(f"ModÃ¨le sauvegardÃ© dans {model_path}")
                                        
                                        # CrÃ©er le dictionnaire d'informations
                                        labels = best_model.predict(scaled_data)
                                        info = {
                                            'n_components': n_components,
                                            'optimize_method': optimize_method,
                                            'scaler': scaler,
                                            'silhouette_score': silhouette_score(scaled_data, labels),
                                            'calinski_harabasz_score': calinski_harabasz_score(scaled_data, labels),
                                            'davies_bouldin_score': davies_bouldin_score(scaled_data, labels),
                                            'bic': best_model.bic(scaled_data),
                                            'aic': best_model.aic(scaled_data)
                                        }
                                        
                                        return best_model, info
                                    
                                    # PrÃ©paration des paramÃ¨tres pour GMM
                                    # Si recherche automatique du nombre optimal, afficher le graphique d'analyse
                                    if params['optimize'] and params['n_components'] is None:
                                        with st.spinner("Recherche du nombre optimal de composantes..."):
                                            # Calcul des scores pour diffÃ©rents nombres de composantes
                                            gmm_scores = {}
                                            
                                            # Standardiser les donnÃ©es
                                            scaler = StandardScaler()
                                            scaled_data_gmm = scaler.fit_transform(data_for_clustering)
                                            
                                            # Ã‰valuer diffÃ©rents nombres de composantes
                                            for n in range(params['n_min'], params['n_max'] + 1):
                                                from sklearn.mixture import GaussianMixture
                                                gmm = GaussianMixture(n_components=n, random_state=42, n_init=params['n_init'])
                                                labels = gmm.fit_predict(scaled_data_gmm)
                                                
                                                # Calculer le score selon la mÃ©thode choisie
                                                if params['scoring_method'] == 'silhouette':
                                                    from sklearn.metrics import silhouette_score
                                                    gmm_scores[n] = silhouette_score(scaled_data_gmm, labels)
                                                elif params['scoring_method'] == 'calinski_harabasz':
                                                    from sklearn.metrics import calinski_harabasz_score
                                                    gmm_scores[n] = calinski_harabasz_score(scaled_data_gmm, labels)
                                                elif params['scoring_method'] == 'davies_bouldin':
                                                    from sklearn.metrics import davies_bouldin_score
                                                    gmm_scores[n] = -davies_bouldin_score(scaled_data_gmm, labels)
                                                elif params['scoring_method'] == 'bic':
                                                    gmm_scores[n] = -gmm.bic(scaled_data_gmm)
                                            
                                            # Affichage du graphique d'analyse
                                            st.subheader(f"Analyse du nombre optimal de composantes - {params['scoring_method']}")
                                            fig, ax = plt.subplots(figsize=(10, 6))
                                            ax.plot(list(gmm_scores.keys()), list(gmm_scores.values()), marker='o')
                                            ax.set_xlabel('Nombre de composantes')
                                            ax.set_ylabel('Score')
                                            ax.set_title(f'Analyse du nombre optimal de composantes - {params["scoring_method"]}')
                                            ax.grid(True)
                                            st.pyplot(fig)
                                            
                                            # SÃ©lection du nombre optimal
                                            optimal_n = max(gmm_scores.items(), key=lambda x: x[1])[0]
                                            st.success(f"Nombre optimal de composantes trouvÃ©: {optimal_n}")
                                    
                                    # Appel de notre fonction personnalisÃ©e avec les paramÃ¨tres explicites
                                    with st.spinner("EntraÃ®nement GMM en cours..."):
                                        model, info_dict = custom_train_gmm(
                                            data=data_for_clustering,
                                            n_components=params['n_components'],
                                            optimize=params['optimize'],
                                            save=params.get('save', True),
                                            n_min=params['n_min'],
                                            n_max=params['n_max'],
                                            scoring_method=params['scoring_method'],
                                            n_init=params['n_init'],
                                            optimize_method=params['optimize_method'],
                                            model_path=params.get('model_path', 'models/gmm_model.pkl'),
                                            max_iter=params.get('max_iter', 100),
                                            init_params=params.get('init_params', 'kmeans'),
                                            cv=params.get('cv', 3),
                                            cov_types=params.get('cov_types', None)
                                        )
                                        
                                        # Mise Ã  jour des attributs de modÃ¨le
                                        model_comp.models['gmm'] = model
                                        info = info_dict
                                        info['training_time'] = time.time() - start_time
                                        model_comp.results['gmm'] = info
                                        
                                        labels = model.predict(scaled_data)
                                
                                training_time = time.time() - start_time
                                st.write(f"Temps total d'entraÃ®nement : {training_time:.2f} s")

                                # 3. Afficher les rÃ©sultats
                                st.subheader("RÃ©sultats du Clustering")
                                if model is not None:
                                    if labels is not None:
                                        n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)
                                        n_noise = list(labels).count(-1)

                                        st.metric("Nombre de clusters trouvÃ©s", n_clusters_found)
                                        if n_noise > 0:
                                             st.metric("Nombre de points de bruit (DBSCAN)", n_noise)

                                        st.metric("Temps d'entraÃ®nement", f"{info.get('training_time', 0):.2f} s")

                                        # Afficher les mÃ©triques (si disponibles et calculÃ©es dans info)
                                        col1, col2, col3 = st.columns(3)
                                        with col1:
                                            st.metric("Score Silhouette", f"{info.get('silhouette_score', 'N/A'):.3f}")
                                        with col2:
                                            st.metric("Score Calinski-Harabasz", f"{info.get('calinski_harabasz_score', 'N/A'):.0f}")
                                        with col3:
                                            st.metric("Score Davies-Bouldin", f"{info.get('davies_bouldin_score', 'N/A'):.3f}")

                                        # Ajouter les labels au DataFrame original (pour exploration potentielle)
                                        # Attention Ã  l'index si des lignes ont Ã©tÃ© supprimÃ©es par dropna
                                        df_cluster_results = data_for_clustering.copy()
                                        df_cluster_results['cluster'] = labels
                                        st.session_state.clustered_data = df_cluster_results # Sauvegarder pour potentiel usage futur


                                        # 4. Visualisation (PCA 2D)
                                        st.subheader("Visualisation des Clusters (PCA 2D)")
                                        if scaled_data.shape[1] >= 2:
                                            pca = PCA(n_components=2, random_state=42)
                                            data_2d = pca.fit_transform(scaled_data)

                                            fig, ax = plt.subplots(figsize=(10, 8))

                                            # Scatter plot avec couleurs par cluster
                                            scatter = ax.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='viridis', alpha=0.6, s=10)

                                            # Distinguer les points de bruit (si DBSCAN)
                                            if n_noise > 0:
                                                 noise_points = data_2d[labels == -1]
                                                 ax.scatter(noise_points[:, 0], noise_points[:, 1], color='red', marker='x', label='Bruit', s=20)

                                            ax.set_title(f"Clusters {algo} visualisÃ©s avec PCA")
                                            ax.set_xlabel("Composante Principale 1")
                                            ax.set_ylabel("Composante Principale 2")
                                            # Ajouter une lÃ©gende si possible/pertinent
                                            # legend1 = ax.legend(*scatter.legend_elements(), title="Clusters")
                                            # ax.add_artist(legend1)
                                            if n_noise > 0:
                                                ax.legend()

                                            st.pyplot(fig)
                                        else:
                                            st.warning("La visualisation PCA nÃ©cessite au moins 2 colonnes sÃ©lectionnÃ©es.")
                                        
                                        # Ajout d'une visualisation 3D avec PCA
                                        st.subheader("Visualisation des Clusters en 3D (PCA)")
                                        if scaled_data.shape[1] >= 3:
                                            from mpl_toolkits.mplot3d import Axes3D
                                            
                                            # RÃ©duire Ã  3 dimensions avec PCA
                                            pca3d = PCA(n_components=3, random_state=42)
                                            data_3d = pca3d.fit_transform(scaled_data)
                                            
                                            # CrÃ©er une figure 3D
                                            fig3d = plt.figure(figsize=(10, 8))
                                            ax3d = fig3d.add_subplot(111, projection='3d')
                                            
                                            # Scatter plot 3D avec couleurs par cluster
                                            scatter3d = ax3d.scatter(
                                                data_3d[:, 0], 
                                                data_3d[:, 1], 
                                                data_3d[:, 2],
                                                c=labels, 
                                                cmap='viridis', 
                                                alpha=0.6,
                                                s=10
                                            )
                                            
                                            # Distinguer les points de bruit (si DBSCAN)
                                            if n_noise > 0:
                                                noise_points_3d = data_3d[labels == -1]
                                                ax3d.scatter(
                                                    noise_points_3d[:, 0],
                                                    noise_points_3d[:, 1],
                                                    noise_points_3d[:, 2],
                                                    color='red',
                                                    marker='x',
                                                    label='Bruit',
                                                    s=20
                                                )
                                                ax3d.legend()
                                            
                                            ax3d.set_title(f"Clusters {algo} visualisÃ©s en 3D avec PCA")
                                            ax3d.set_xlabel("Composante Principale 1")
                                            ax3d.set_ylabel("Composante Principale 2")
                                            ax3d.set_zlabel("Composante Principale 3")
                                            
                                            # Ajouter une barre de couleur
                                            cbar = fig3d.colorbar(scatter3d, ax=ax3d, pad=0.1)
                                            cbar.set_label('Clusters')
                                            
                                            st.pyplot(fig3d)
                                        elif scaled_data.shape[1] == 2:
                                            st.warning("La visualisation 3D nÃ©cessite au moins 3 colonnes sÃ©lectionnÃ©es. Actuellement vous n'avez que 2 colonnes.")
                                        else:
                                            st.warning("La visualisation 3D nÃ©cessite au moins 3 colonnes sÃ©lectionnÃ©es.")

                                        # Afficher un aperÃ§u des donnÃ©es avec les clusters assignÃ©s
                                        st.subheader("AperÃ§u des donnÃ©es avec clusters")
                                        st.dataframe(df_cluster_results.head())
                                        
                                        # Analyse des caractÃ©ristiques des clusters
                                        st.subheader("Analyse des clusters")
                                        
                                        # RÃ©cupÃ©rer le DataFrame original avec toutes les caractÃ©ristiques
                                        # Utiliser data_for_clustering pour les caractÃ©ristiques et ajouter les labels
                                        df_with_clusters = data_for_clustering.copy()
                                        df_with_clusters['cluster'] = labels
                                        
                                        # DÃ©terminer le nombre de clusters
                                        unique_clusters = sorted(df_with_clusters['cluster'].unique())
                                        # Exclure le cluster -1 (bruit) s'il existe
                                        if -1 in unique_clusters:
                                            unique_clusters.remove(-1)
                                            st.info(f"Cluster -1 (points de bruit) exclu de l'analyse des caractÃ©ristiques.")
                                        
                                        # Pour chaque cluster, dÃ©terminer les 5 caractÃ©ristiques les plus importantes
                                        for cluster_id in unique_clusters:
                                            st.write(f"**Cluster {cluster_id}** ({(df_with_clusters['cluster'] == cluster_id).sum()} points)")
                                            
                                            # SÃ©lectionner les donnÃ©es de ce cluster
                                            cluster_data = df_with_clusters[df_with_clusters['cluster'] == cluster_id]
                                            
                                            # MÃ©thode 1: CaractÃ©ristiques avec les moyennes les plus Ã©levÃ©es
                                            means = cluster_data.mean().sort_values(ascending=False)
                                            st.write("CaractÃ©ristiques avec les moyennes les plus Ã©levÃ©es:")
                                            st.write(means.head(5))
                                            
                                            # MÃ©thode 2: CaractÃ©ristiques les plus distinctives (diffÃ©rence avec la moyenne globale)
                                            global_means = df_with_clusters.mean()
                                            diff = (cluster_data.mean() - global_means).abs().sort_values(ascending=False)
                                            st.write("CaractÃ©ristiques les plus distinctives:")
                                            st.write(diff.head(5))
                                            
                                            # Ajouter une ligne de sÃ©paration entre les clusters
                                            st.markdown("---")
                                        
                                        # Si des mÃ©tadonnÃ©es ou des libellÃ©s sont disponibles, ajouter une analyse plus descriptive
                                        st.info("Conseil: Pour une analyse plus dÃ©taillÃ©e, utilisez les mÃ©tadonnÃ©es ou les libellÃ©s associÃ©s aux caractÃ©ristiques numÃ©riques.")

                                    else:
                                        st.error("Le modÃ¨le n'a pas retournÃ© de labels.")
                                else:
                                     st.error("L'entraÃ®nement du modÃ¨le a Ã©chouÃ©.")

                            except Exception as e:
                                st.error(f"Une erreur est survenue lors du clustering : {e}")
                                st.exception(e) # Afficher la trace pour dÃ©bogage 